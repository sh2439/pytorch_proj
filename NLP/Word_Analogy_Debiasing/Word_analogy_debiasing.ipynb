{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Analogy and Debiasing\n",
    "---\n",
    "This notebook contains word analogy, debiasing and equalizing taks. With the help of modern word embbeddings (e.g. GloVe, word2vec), we are able to make use of word vectors and accomplish these tasks.\n",
    "1. **Word Analogy:** Compute word analogy. For example, 'China' is to 'Mandarin' as 'France' is to 'French'.\n",
    "2. **Debiasing:** The dataset which was used to train the word embeddings can reflect the some bias of human language. Gender bias is a significant one. \n",
    "3. **Equalizing:** Some words are gender-specific. For example, we may assume gender is the only difference between 'girl' and 'boy'. Therefore, they should have the same distance from other dimensions.\n",
    "\n",
    "### Acknowledgement:\n",
    "Some ideas come from [Deep Learning Course on Coursera](https://www.deeplearning.ai/deep-learning-specialization/) (e.g., the debiasing and equalizing equations) and the [paper](https://arxiv.org/abs/1607.06520)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Word Embeddings\n",
    "The pre-trained word vectors is downloaded from [GloVe](https://nlp.stanford.edu/projects/glove/). The file I used contains 400k words and 50 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the GloVe text file and return the words.\n",
    "def read_glove(name):\n",
    "    \"\"\"Given the path/name of the glove file, return the words(set) and word2vec_map(a python dict)\n",
    "    \"\"\"\n",
    "    file = open(name, 'r')\n",
    "    # Create set for words and a dictionary for words and their corresponding  \n",
    "    words = set()\n",
    "    word2vec_map = {}\n",
    "    \n",
    "    data = file.readlines()\n",
    "    for line in data:\n",
    "        # add word to the words set.\n",
    "        word = line.split()[0]\n",
    "        words.add(word)\n",
    "        \n",
    "        word2vec_map[word] = np.array(line.split()[1:], dtype = np.float64)\n",
    "        \n",
    "    return words, word2vec_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocab: 400000\n",
      "dimension of word: (50,)\n"
     ]
    }
   ],
   "source": [
    "words, word2vec_map =  read_glove('glove.6B.50d.txt')    \n",
    "\n",
    "# length of vocab\n",
    "print('length of vocab:',len(words))\n",
    "\n",
    "# dimension of word\n",
    "print('dimension of word:',word2vec_map['hello'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Analogy\n",
    "### 2.1 Define similarity\n",
    "Cosine similarity is used to measure the similarity of two vectors. \n",
    "$$\\text{Cosine Similarity(a, b)} = \\frac {a . b} {||a||_2 ||b||_2} = cos(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a,b):\n",
    "    \"\"\"Given vector a and b, compute the cosine similarity of these two vectors.\n",
    "    \"\"\"\n",
    "    # Compute the dot product of a,b\n",
    "    dot = np.dot(a,b)\n",
    "    # compute the cosine similarity of a,b\n",
    "    sim = dot/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    \n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8860337718495819\n"
     ]
    }
   ],
   "source": [
    "print(cosine_sim(word2vec_map['man'], word2vec_map['woman']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Find word analogy\n",
    "If word a is to b as c is to d. Then, we have: $e_b - e_a \\approx e_d - e_c$. Iterate over the vocabulary to find the best word analogy given three words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(word_a, word_b, word_c, words, word2vec):\n",
    "    \"\"\"word_a is to word_b as word_c is to __.\n",
    "    Find the word given the words and word vectors.\n",
    "    \"\"\"\n",
    "    # Make sure the inputs are in lower case.\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    a,b,c = word2vec[word_a], word2vec[word_b], word2vec[word_c]\n",
    "    \n",
    "    best_sim = -100\n",
    "    best_word = None\n",
    "    for word in words:\n",
    "        \n",
    "        if word in [word_a, word_b, word_c]:\n",
    "            continue\n",
    "        # compute the current similarity\n",
    "        sim = cosine_sim(a-b, c-word2vec[word])\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_word = word\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: good -> better\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'good')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, word_analogy(*triad,words, word2vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Debiasing\n",
    "Some words should be neutral to the gender. But pre-trained word vectors are not, which reflects the language bias when we are using the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define the gender vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16192108462558177\n",
      "-0.0939532553641572\n"
     ]
    }
   ],
   "source": [
    "g1 = word2vec_map['man'] - word2vec_map['woman']\n",
    "g2 = word2vec_map['father'] - word2vec_map['mother']\n",
    "g3 = word2vec_map['boy'] - word2vec_map['girl']\n",
    "# Average the subtractions.\n",
    "g = (g1+g2+g3)/3\n",
    "\n",
    "print(cosine_sim(word2vec_map['technology'], g))\n",
    "print(cosine_sim(word2vec_map['flower'], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Neutralize the words\n",
    "Here is the equation to neutralize the words. \n",
    "\n",
    "$$e^{bias\\_component} = \\frac{e \\cdot g}{||g||_2^2} * g$$\n",
    "$$e^{debiased} = e - e^{bias\\_component}$$\n",
    "\n",
    "Where:  \n",
    "$g$: The gender vector.  \n",
    "$e$: The original word vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word, gender, word2vec):\n",
    "    \"\"\"Given the word to neutralize, gender vector and the word vectors, neutralize the word.\n",
    "    \"\"\"\n",
    "    e = word2vec[word]\n",
    "    e_bias = (np.dot(e,gender)/(np.linalg.norm(gender)**2))*gender\n",
    "    \n",
    "    e_unbiased = e - e_bias\n",
    "    return e_unbiased\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After neutralizing words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8444594232094444e-17\n",
      "-8.244955165656526e-18\n"
     ]
    }
   ],
   "source": [
    "print(cosine_sim(g,neutralize('technology', g, word2vec_map) ))\n",
    "print(cosine_sim(g,neutralize('flower', g, word2vec_map) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Equalizing \n",
    "Some gender-specific words should be equidistant from non-gender dimensions(axis). \n",
    "\n",
    "Major equations:\n",
    "$$ \\mu = \\frac{e_{w1} + e_{w2}}{2}$$ \n",
    "\n",
    "$$ \\mu_{B} = \\frac {\\mu \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}$$ \n",
    "\n",
    "$$\\mu_{\\perp} = \\mu - \\mu_{B} $$\n",
    "\n",
    "$$ e_{w1B} = \\frac {e_{w1} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
    "$$ \n",
    "$$ e_{w2B} = \\frac {e_{w2} \\cdot \\text{bias_axis}}{||\\text{bias_axis}||_2^2} *\\text{bias_axis}\n",
    "$$\n",
    "\n",
    "\n",
    "$$e_{w1B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w1B}} - \\mu_B} {|(e_{w1} - \\mu_{\\perp}) - \\mu_B)|} $$\n",
    "\n",
    "\n",
    "$$e_{w2B}^{corrected} = \\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |} * \\frac{e_{\\text{w2B}} - \\mu_B} {|(e_{w2} - \\mu_{\\perp}) - \\mu_B)|} $$\n",
    "\n",
    "$$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp} $$\n",
    "$$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis, word2vec_map):\n",
    "    \"\"\"Given the word pairs, the bias axis and the word vectors, \n",
    "       make the word pairs equidistant from unbiased axis.\n",
    "    \"\"\"\n",
    "\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word2vec_map[w1], word2vec_map[w2]\n",
    "    \n",
    "    # Compute the mean of e_w1 and e_w2\n",
    "    mu = (e_w1+e_w2)/2\n",
    "\n",
    "    # Compute the projections of mu over the bias axis and the orthogonal axis\n",
    "    mu_B = np.dot(mu,bias_axis)/(np.square(np.linalg.norm(bias_axis)))*bias_axis\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Compute e_w1B and e_w2B \n",
    "    e_w1B = np.dot(e_w1,bias_axis)/(np.square(np.linalg.norm(bias_axis)))*bias_axis\n",
    "    e_w2B = np.dot(e_w2,bias_axis)/(np.square(np.linalg.norm(bias_axis)))*bias_axis\n",
    "        \n",
    "    # Adjust the Bias part of e_w1B and e_w2B\n",
    "    corrected_e_w1B = np.sqrt(np.abs(1-np.square(np.linalg.norm(mu_orth))))*(e_w1B-mu_B)/np.linalg.norm((e_w1-mu_orth)-mu_B)\n",
    "    corrected_e_w2B = np.sqrt(np.abs(1-np.square(np.linalg.norm(mu_orth))))*(e_w2B-mu_B)/np.linalg.norm((e_w2-mu_orth)-mu_B)\n",
    "\n",
    "    # Debias by equalizing e1 and e2 to the sum of their corrected projections\n",
    "    e1 = corrected_e_w1B + mu_orth\n",
    "    e2 = corrected_e_w2B + mu_orth\n",
    "\n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"man\"], gender) =  0.02435875412347579\n",
      "cosine_similarity(word_to_vec_map[\"woman\"], gender) =  -0.3979047171251496\n",
      "\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  0.6624273110383183\n",
      "cosine_similarity(e2, gender) =  -0.6624273110383184\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_sim(word2vec_map[\"man\"], g))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_sim(word2vec_map[\"woman\"], g))\n",
    "print()\n",
    "e1, e2 = equalize((\"man\", \"woman\"), g, word2vec_map)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_sim(e1, g))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_sim(e2, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
